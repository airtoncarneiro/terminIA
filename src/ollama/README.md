# ğŸ³ Executando Ollama dentro de um Container Docker

Este guia explica como instalar, buildar e executar o **Ollama** dentro
de um container Docker, permitindo que vocÃª use modelos LLM atravÃ©s da
API local em `http://localhost:11434`.

------------------------------------------------------------------------

## ğŸ“Œ Requisitos

-   Docker instalado\
-   (Opcional) Docker Compose instalado\
-   Sistema operacional: Linux, macOS ou Windows

------------------------------------------------------------------------

# ğŸš€ 1. Clonar ou criar o diretÃ³rio do projeto

``` bash
mkdir ollama-docker
cd ollama-docker
```

------------------------------------------------------------------------

# ğŸš€ 2. Criar os arquivos

Crie os arquivos abaixo:

### **Dockerfile**

``` dockerfile
FROM ollama/ollama:latest

RUN mkdir -p /root/.ollama

EXPOSE 11434

CMD ["serve"]
```

### **docker-compose.yml (opcional)**

``` yaml
version: '3.9'

services:
  ollama:
    build: .
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama

volumes:
  ollama_models:
```

------------------------------------------------------------------------

# ğŸ—ï¸ 3. Build da imagem

### Usando Docker diretamente:

``` bash
docker build -t my-ollama .
```

### Usando Docker Compose:

``` bash
docker compose build
```

------------------------------------------------------------------------

# â–¶ï¸ 4. Executar o servidor Ollama

### Via Docker:

``` bash
docker run -d --name ollama \
  -p 11434:11434 \
  -v ollama_models:/root/.ollama \
  my-ollama
```

### Via Docker Compose:

``` bash
docker compose up -d
```

------------------------------------------------------------------------

# ğŸ” 5. Testar se o servidor estÃ¡ funcionando

``` bash
curl http://localhost:11434/api/version
```

Resposta esperada:

``` json
{"version":"0.3.x"}
```

------------------------------------------------------------------------

# ğŸ“¥ 6. Baixar um modelo dentro do container

``` bash
docker exec -it ollama ollama pull phi3:mini

docker exec -it ollama run deepseek-r1:1.5b
```

## para listar os modelos baixados
``` bash
docker exec -it ollama ollama list
```

## para remover um modelo
``` bash
docker exec -it ollama rm qwen2.5:4b
```

------------------------------------------------------------------------

# ğŸ¤– 7. Usar um modelo

``` bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi3:mini",
  "prompt": "Explique buffer overflow."
}'
```

------------------------------------------------------------------------

# ğŸ›‘ 8. Parar e remover o container

### Via docker:

``` bash
docker stop ollama
docker rm ollama
```

### Via docker-compose:

``` bash
docker compose down
```

------------------------------------------------------------------------

# ğŸ’¾ Onde ficam os modelos?

Eles sÃ£o salvos dentro do volume Docker:

    ollama_models:/root/.ollama

Isso garante persistÃªncia mesmo apÃ³s remover ou rebuildar o container.

------------------------------------------------------------------------

# ğŸ¯ ConclusÃ£o

Agora vocÃª tem:

âœ” Ollama containerizado\
âœ” PersistÃªncia dos modelos\
âœ” API exposta em `localhost:11434`\
âœ” Pronto para integrar com qualquer app (Python, JS, PHP, Postman etc.)

Se quiser, posso gerar tambÃ©m:

-   exemplo de consumo via **PHP**\
-   exemplo via **Python**\
-   versÃ£o **com GPU (NVIDIA)** usando `--gpus all`\
-   versÃ£o **pronta para deploy em VPS**\
-   versÃ£o **com tÃºnel Cloudflare para acesso remoto**

# Executar o LMStudio: `/Downloads/LM-Studio-0.3.32-2-x64.AppImage --no-sandbox`